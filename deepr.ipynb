{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0307e9",
   "metadata": {},
   "source": [
    "# CS598 Deep Learning for Healthcare Final Project\n",
    "## Reproduction of Deepr: A Convolutional Net for Medical Records\n",
    "### Juan Alvarez Martinez, Shane Sepac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Include summary and report of findings here [200 words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0ba21",
   "metadata": {},
   "source": [
    "## Load MIMIC-III Dataset. \n",
    "Several csv files are needed from the MIMIC-III dataset: ADMISSIONS, PATIENTS, DIAGNOSES_ICD, and PROCEDURES_ICD. These files can be loaded automatically out of S3, or you can place them in `<project_root>/mimic3`. \n",
    "- If loading out of S3, ensure you have all environment variables from .env.sample copied and instantiated in a .env file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4700fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required dependencies\n",
    "%pip install boto3 python-dotenv pandas pyhealth matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383fc65",
   "metadata": {},
   "source": [
    "### Get MIMIC-3 Data\n",
    "Attempt to load MIMIC-3 data out of S3 if the relevant CSV files are not already in the mimic3 folder at the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import copy_file_from_s3\n",
    "\n",
    "data_folder = \"mimic3\"\n",
    "required_files = [\"ADMISSIONS.csv\", \"PATIENTS.csv\", \"DIAGNOSES_ICD.csv\", \"PROCEDURES_ICD.csv\", \"TRANSFERS.csv\"]\n",
    "\n",
    "for i, fn in enumerate(required_files):\n",
    "  if not os.path.exists(f\"{data_folder}/{fn}\"):\n",
    "    print(f\"Cannot find {fn} in {data_folder}, trying to download from S3...\")\n",
    "    copy_file_from_s3(fn, data_folder)\n",
    "  else:\n",
    "    print(f\"Found {fn}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9202ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\"./mimic3/\", [\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"]) #pyhealth does not support mapping ICD-9 to ICD-10 codes.\n",
    "\n",
    "mimic3_ds.info()\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3a35f",
   "metadata": {},
   "source": [
    "## Sequencing EMR: Creating Sentences representing patient episodes\n",
    "Per Deepr, an EMR must be translated into a sentence for use downstream the model. An EMR is a sequence of time-stamped visit episodes. Each episode involves a series of diagnoses and treatments, called a phrase. Each phrase is separated by a time interval equal to `(0–1], (1–3], (3–6], (6–12], and 12+` or `TRANSFER`, with the latter indicating a transfer between care providers (separate departments within the same hospital or between hospitals.) Infrequent words are coded with `RAREWORD`, which indicates the word has appeared <100 times. Per the Deepr paper, an example sentence looks as follows:\n",
    "\n",
    "```\n",
    "1910 Z83 911 1008 D12 K31 1-3m R94 RAREWORD H53 Y83 M62 Y92 E87 T81 RAREWORD RAREWORD 1893 D12 S14 738 1910 1916 Z83 0-1m T91 RAREWORD Y83 Y92 K91 M10 E86 6-12m K31 1008 1910 Z13 Z83.\n",
    "```\n",
    "\n",
    "Note: In the sentence above, diagnoses are in ICD-10 format (a character followed by digits) and procedures are in digits. \n",
    "\n",
    "The MIMIC-3 dataset provides ICD-9 codes, and these will be used, but the level-3 variant of them for consistency with the original paper. It can also be noted that the encounter and discharge datetimes for visits are between the years 2100-2200 in order to deidentify patients, however, the time interval between visits is indeed preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852caebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find rare words (diagnoses and procedures with counts of less than 100)\n",
    "'''\n",
    "word_cnts = {}\n",
    "\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "  words = []\n",
    "\n",
    "  for _, v in p.visits.items():\n",
    "    for e in v.get_event_list('DIAGNOSES_ICD'):\n",
    "      words.append(e.code)\n",
    "\n",
    "    for e in v.get_event_list('PROCEDURES_ICD'):\n",
    "      words.append(e.code)\n",
    "\n",
    "  for word in words:\n",
    "    # If the word is already in the dictionary, increment the count\n",
    "    if word in word_cnts:\n",
    "        word_cnts[word] += 1\n",
    "    # Otherwise, add the word to the dictionary with a count of 1\n",
    "    else:\n",
    "        word_cnts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d19407",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Append to pyhealth's record of visits so that ADMISSION_TYPE and ADMISSION_LOCATION data are available. The former is used later in the model training\n",
    "to target against readmission (by looking at non-elective admits) and the latter is used to discern intra/inter hospital transfers, which is needed to build \n",
    "the sentences already described (i.e. to help build sentences using the TRANSFER keyword.)\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "admissions_df = pd.read_csv(\"./mimic3/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.set_index(\"HADM_ID\")\n",
    "\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "  # Sort patient visits by encounter_time\n",
    "  for i, v in enumerate(p.visits.items(),):\n",
    "    res = admissions_df.loc[int(v[1].visit_id)]\n",
    "\n",
    "    v[1].attr_dict[\"ADMISSION_TYPE\"] = res[\"ADMISSION_TYPE\"]\n",
    "    v[1].attr_dict[\"ADMISSION_LOCATION\"] = res[\"ADMISSION_LOCATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import timedelta_to_interval\n",
    "import random\n",
    "import json\n",
    "\n",
    "'''\n",
    "Translate EMRs into sentences outlined by the paper. A sentence consists of phrases, which are randomly shuffled diagnosis and procedure codes, separated by the \n",
    "time interval between visits, if the time interval exists. Sentences should have 100 words max.\n",
    "\n",
    "While looping over each patient:\n",
    "  1. Sort visits by encounter_time\n",
    "  2. Find the time interval between each visit and generate its relevant string word\n",
    "  3. Build arrays of diagnosis and procedure codes for each visit, replacing ICD-10 codes with less than 100 usages with RAREWORD\n",
    "  4. Randomly shuffle each array of diagnosis and procedure codes, then append the time interval string if available. This represents a phrase.\n",
    "    Concat each phrase to an array, which will be concatenated to form the final sentence. If the concatenation would form a sentence longer\n",
    "    than 100 words, min(100, words(sentence)) is adhered to.\n",
    "'''\n",
    "sentences = []\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "\n",
    "  # Sort patient visits by encounter_time\n",
    "  sorted_visits = sorted(p.visits.items(), key=lambda v: v[1].encounter_time) # sort by encounter time in order to guage interval between visits\n",
    "\n",
    "  # Generate timestamps in between visits\n",
    "  discharge_times = [visit[1].discharge_time for visit in sorted_visits[:-1]]\n",
    "  encounter_times = [visit[1].encounter_time for visit in sorted_visits[1:]]\n",
    "\n",
    "  time_intervals = [\n",
    "      t2 - t1\n",
    "      for t1, t2 in zip(discharge_times, encounter_times)\n",
    "  ]\n",
    "\n",
    "  # Convert timestamps to month intervals as specified in paper\n",
    "  time_interval_strs = timedelta_to_interval(time_intervals)\n",
    "\n",
    "  # event_diagnoses_ls = (visit, diagnoses_codes)\n",
    "  event_diagnoses_ls = []\n",
    "\n",
    "  # event_procedures_ls = (visit, procedure_codes)\n",
    "  event_procedures_ls = []\n",
    "\n",
    "  # Helper function to create arrays with RAREWORD using list comprehension\n",
    "  def handle_event(event_list, word_cnts):\n",
    "      return [\"RAREWORD\" if e.code in word_cnts and word_cnts[e.code] < 100 else e.code for e in event_list]\n",
    "\n",
    "  # build arrays of diagnoses and procedures on a visit level, add to event_diagnoses_ls or event_procedures_ls\n",
    "  for _, v in sorted_visits:\n",
    "      visit_diagnoses = handle_event(v.get_event_list('DIAGNOSES_ICD'), word_cnts)\n",
    "      event_diagnoses_ls.append(visit_diagnoses)\n",
    "\n",
    "      visit_procedures = handle_event(v.get_event_list('PROCEDURES_ICD'), word_cnts)\n",
    "      event_procedures_ls.append(visit_procedures)\n",
    "\n",
    "\n",
    "  # Randomly shuffle diagnosis and procedure codes and append a time interval after, if available. Ensure the output sentence will not be more than 100 words.\n",
    "  arrs = []\n",
    "  word_cnt = 0\n",
    "  for i, vd in enumerate(event_diagnoses_ls):\n",
    "      arr = vd + event_procedures_ls[i]\n",
    "      random.shuffle(arr)\n",
    "      if i < len(time_interval_strs):\n",
    "          arr.append(time_interval_strs[i])\n",
    "\n",
    "      new_word_cnt = word_cnt + len(arr)\n",
    "\n",
    "      if new_word_cnt > 100:\n",
    "          # Calculate the number of elements needed to reach exactly 100 words\n",
    "          elements_needed = 100 - word_cnt\n",
    "\n",
    "          # Take a subset of arr to make new_word_cnt equal 100\n",
    "          arr = arr[:elements_needed]\n",
    "          arrs.append(arr)\n",
    "          break\n",
    "\n",
    "      arrs.append(arr)\n",
    "      word_cnt = new_word_cnt\n",
    "\n",
    "  # Combine all codes and time interval to create a phrase, representing a visit\n",
    "  phrases = [\" \".join(arr) for arr in arrs]\n",
    "\n",
    "  # Combine all phrases to create a sentence, representing a sequence as outlined by the paper\n",
    "  sentence = \" \".join(phrases)\n",
    "  sentences.append(sentence)\n",
    "\n",
    "# output to json file\n",
    "output_dir = \"data\"\n",
    "output_filename = \"sentences.json\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, output_filename), \"w\") as json_file:\n",
    "  json.dump(sentences, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6957a",
   "metadata": {},
   "source": [
    "### Test that output sentences satisfy the following conditions:\n",
    "- There is a sentence for each patient\n",
    "- Each sentence is capped to max 100 words\n",
    "- Multi visit patients have visits separated by a timestamp\n",
    "- Words should not exist in their ICD-10 form if used less than 100 times (should be replaced with RAREWORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b42736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# There should be one sentence per patient\n",
    "num_patients = len(mimic3_ds.patients)\n",
    "num_sentences = len(sentences)\n",
    "assert(num_patients == num_sentences)\n",
    "\n",
    "# There should be max 100 words per sentence\n",
    "word_lengths = map(lambda s: len(s.split()), sentences)\n",
    "assert(max(list(word_lengths)) <= 100)\n",
    "\n",
    "# There should be no word in any of the sentences that is present less than 100 times\n",
    "rarewords = [word for word, count in word_cnts.items() if count < 100]\n",
    "for sentence in sentences:\n",
    "  words_of_sentence = sentence.split()\n",
    "  rareword_violations = list(filter(lambda w: w in word_cnts and word_cnts[w] < 100, words_of_sentence))\n",
    "  assert(len(rareword_violations) == 0)\n",
    "\n",
    "\n",
    "# Patients with multiple visits should have timestamps separating their visits i.e. 1-3m or 12+m #TODO: Add TRANSFER to regex\n",
    "pattern = re.compile(r\"[-+]\")\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "    if len(p.visits) > 1:\n",
    "        if not pattern.search(sentences[i]):\n",
    "            print(f\"Failed assertion for sentences[{i}]: '{sentences[i]}'\")\n",
    "            assert(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f873178",
   "metadata": {},
   "source": [
    "## Training Word2Vec\n",
    "#TODO: Write description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdace4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def split_tokens(stemmer, stopwords, line):\n",
    "    return [\n",
    "        stemmer.stem(i)\n",
    "        for i in re.split(r\" +\", re.sub(r\"[^a-z@# ]\", \"\", line.lower()))\n",
    "        if (i not in stopwords) and len(i)\n",
    "    ]\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "sentences = [\n",
    "    \" \".join(sentence)\n",
    "    for line in open(\"data/shakespeare.txt\", \"r\").readlines()\n",
    "    if (sentence := split_tokens(stemmer, stopwords, line)) and sentence\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset, tok2id, wsize=3):\n",
    "        self.wsize = wsize\n",
    "        self.tok2id = tok2id\n",
    "\n",
    "        self.dataset = [\n",
    "            ctx\n",
    "            for sentence in dataset for ctx in self.get_contexts(sentence)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def get_contexts(self, sentence):\n",
    "        moving_window = []\n",
    "\n",
    "        for word_ix, word in enumerate(sentence):\n",
    "            target = self.tok2id[word]\n",
    "            window = [\n",
    "                word_ix + win_ix\n",
    "                for win_ix in range(-self.wsize, self.wsize + 1)\n",
    "                if (\n",
    "                    word_ix + win_ix >= 0\n",
    "                    and word_ix + win_ix < len(sentence)\n",
    "                    and win_ix != 0\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            moving_window += [\n",
    "                (target, self.tok2id[sentence[win_word]]) for win_word in window\n",
    "            ]\n",
    "\n",
    "        return moving_window\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.prediction = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.embedding(input)\n",
    "        logits = self.prediction(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "dataset = [sentence.split(\" \") for sentence in sentences]\n",
    "vocab = set([word for sentence in dataset for word in sentence])\n",
    "\n",
    "id2tok = dict(enumerate(vocab))\n",
    "tok2id = {token: id for id, token in id2tok.items()}\n",
    "\n",
    "word2vec_dataloader = DataLoader(\n",
    "    Word2VecDataset(dataset, tok2id),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "word2vec_model = Word2Vec(len(vocab), embedding_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "LR = 3e-4\n",
    "EPOCHS = 90\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(word2vec_model.parameters(), lr=LR)\n",
    "progress_bar = tqdm(range(EPOCHS * len(word2vec_dataloader)))\n",
    "\n",
    "running_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for center, context in word2vec_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = word2vec_model(input=context)\n",
    "        loss = loss_fn(logits, center)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    epoch_loss /= len(word2vec_dataloader)\n",
    "    running_loss.append(epoch_loss)\n",
    "\n",
    "plt.plot(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_distance_matrix(wordvecs, metric):\n",
    "    dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def get_k_similar_words(word, dist_matrix, k=10):\n",
    "    idx = tok2id[word]\n",
    "    dists = dist_matrix[idx]\n",
    "    ind = np.argpartition(dists, k)[:k+1]\n",
    "    ind = ind[np.argsort(dists[ind])][1:]\n",
    "    out = [(i, id2tok[i], dists[i]) for i in ind]\n",
    "    return out\n",
    "\n",
    "\n",
    "dmat = get_distance_matrix(\n",
    "    word2vec_model.prediction.weight.cpu().detach().numpy(),\n",
    "    \"cosine\"\n",
    ")\n",
    "\n",
    "test_sentence = [\"good\", \"father\" \"school\", \"hate\"]\n",
    "\n",
    "for word in test_sentence:\n",
    "    print(word, [token[1] for token in get_k_similar_words(word, dmat)], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec_model.state_dict()\n",
    "torch.save(weights, os.path.join(output_dir, \"word2vec.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef13fa",
   "metadata": {},
   "source": [
    "## Convolutional Network\n",
    "Now that Word2Vec has created embeddings for the sentences we generated in the first step, we are ready to pass the embeddings through a convolutional layer, followed by max pooling.\n",
    "\n",
    "First, we will generate the train and val loaders. This will first require us to create the validation set. In Deepr, we try to predict the likelihood of unplanned re-admission after both 3 and 6 months. Unplanned re-admission is coded in the database as an unplanned or emergency with the status not equal to \"elective\" or a transfer from another hospital.\n",
    "\n",
    "Next, we will define a model containing a convolutional network, ReLU and max pooling layer.\n",
    "\n",
    "Deepr reported the following parameters were determined to be optimal for the model:\n",
    "\n",
    "`m = 100, d = 1, motif size = 3, 4, and 5, n = 100 number of epochs = 10, minibatch size = 64, and l2 regularization λ = 1.0.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create validation dataset, train and val loaders for training Deepr.\n",
    "'''\n",
    "\n",
    "val_readmission_time_days = 180 # The threshold for flagging unplanned readmission for validation set\n",
    "candidate_group = []\n",
    "risk_group = []\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "\n",
    "  # Sort patient visits by encounter_time\n",
    "  sorted_visits = sorted(p.visits.items(), key=lambda v: v[1].encounter_time) # sort by encounter time in order to guage interval between visits\n",
    "\n",
    "  # Generate timestamps in between visits\n",
    "  discharge_times = [visit[1].discharge_time for visit in sorted_visits[:-1]]\n",
    "  encounter_times = [visit[1].encounter_time for visit in sorted_visits[1:]]\n",
    "\n",
    "  time_intervals = [\n",
    "      t2 - t1\n",
    "      for t1, t2 in zip(discharge_times, encounter_times)\n",
    "  ]\n",
    "  visit_types = [visit[1].attr_dict[\"ADMISSION_TYPE\"] for visit in sorted_visits[1:]]\n",
    "\n",
    "  unplanned_readmissions = [\n",
    "      i_vt\n",
    "      for i_vt, (interval, visit_type) in enumerate(zip(time_intervals, visit_types))\n",
    "      if interval.days <= val_readmission_time_days and visit_type == \"EMERGENCY\"\n",
    "  ]\n",
    "\n",
    "  if unplanned_readmissions:\n",
    "    risk_group.append(sentences[i])\n",
    "  else: \n",
    "    candidate_group.append(sentences[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk group patients should be separate from the candidate group, and sum to the length of the total dataset\n",
    "assert(len(mimic3_ds.patients.values()) == len(risk_group) + len(candidate_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "'''\n",
    "Returns array of word embeddings corresponding to the sentences in the training data.\n",
    "'''\n",
    "class DeeprDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, word_embeddings_dict):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.word_embeddings_dict = word_embeddings_dict\n",
    "\n",
    "    def get_embedding(self, sentence):\n",
    "      word_tensors = [torch.tensor(self.word_embeddings_dict[word]) for word in sentence.split()]\n",
    "      return torch.stack(word_tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    # returns an array of the embeddings of each word in the sentence, shape of (words(sentenc), embedding_dim)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_embedding(self.x_data[idx]), self.get_embedding(self.y_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_ratio = 0.8\n",
    "random.shuffle(candidate_group)\n",
    "random.shuffle(risk_group)\n",
    "\n",
    "word_embeddings_dict = {word: word2vec_model.embedding(torch.tensor(tok2id[word])).detach().numpy() for word in tok2id}\n",
    "\n",
    "train_size = int(len(risk_group) * train_ratio)\n",
    "x_train_data = candidate_group[:train_size]\n",
    "x_val_data = candidate_group[train_size:]\n",
    "y_train_data = risk_group[:train_size]\n",
    "y_val_data = risk_group[train_size:]\n",
    "\n",
    "train_dataset = DeeprDataset(x_train_data, y_train_data, word_embeddings_dict)\n",
    "val_dataset = DeeprDataset(x_val_data, y_val_data, word_embeddings_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e270c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED\n",
    "class Deepr(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Conv1d()\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.pool = nn.MaxPool2d()\n",
    "\n",
    "    def forward(self, input):\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED\n",
    "# Train the model: use Cross Entropy Loss loss fxn and SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83820533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED\n",
    "# Evaluate the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
