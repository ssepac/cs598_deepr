{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee0307e9",
   "metadata": {},
   "source": [
    "# CS598 Deep Learning for Healthcare Final Project\n",
    "## Reproduction of Deepr: A Convolutional Net for Medical Records\n",
    "### Juan Alvarez Martinez, Shane Sepac"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ec06a5b",
   "metadata": {},
   "source": [
    "## Reproducibility Summary\n",
    "We implemented *Deepr: A Convolutional Net for Medical Records* and achieved an AUC in the model validation consistent and if not better than the original paper reported. The MIMIC-3 dataset was used for testing against the Deepr paper. The python library *Pyhealth* was used to ingest the MIMIC-3 dataset in order to leverage patient, visit and event level data. These data were mapped into \"sentences,\" as referenced in the paper, which were formed by capturing the ICD-9 diagnoses and procedure codes for events within a given visit into \"phrases\", and each phrase concatenated between a timestamp indicating duration between patient visits. After generating sentences according to every patient's EMR records, we generated word embeddings for each sentence via Word2Vec (skip-gram), which was leveraged to help capture the context of surrounding words. The embeddings were pushed through our Deepr model implementation, which consists of convolutional, relu, max pooling and linear classification layers, which produced a binary classification of unplanned readmission likelihood. The data used to train the model was a 1-1 mixture of unplanned readmitted and non-readmitted patients. During validation, we achieved results consistent with the Deepr paper, with an AUC of ~0.85."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36b0ba21",
   "metadata": {},
   "source": [
    "## Load MIMIC-III Dataset. \n",
    "Several csv files are needed from the MIMIC-III dataset: ADMISSIONS, PATIENTS, DIAGNOSES_ICD, and PROCEDURES_ICD. These files can be loaded automatically out of S3, or you can place them in `<project_root>/mimic3`. \n",
    "- If loading out of S3, ensure you have all environment variables from .env.sample copied and instantiated in a .env file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4700fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/homebrew/lib/python3.11/site-packages (1.26.109)\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: pyhealth in /opt/homebrew/lib/python3.11/site-packages (1.1.3)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.109 in /opt/homebrew/lib/python3.11/site-packages (from boto3) (1.29.109)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/homebrew/lib/python3.11/site-packages (from pyhealth) (2.0.0)\n",
      "Requirement already satisfied: rdkit>=2022.03.4 in /opt/homebrew/lib/python3.11/site-packages (from pyhealth) (2022.9.5)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /opt/homebrew/lib/python3.11/site-packages (from pyhealth) (1.2.2)\n",
      "Requirement already satisfied: networkx>=2.6.3 in /opt/homebrew/lib/python3.11/site-packages (from pyhealth) (3.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from pyhealth) (4.65.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/homebrew/lib/python3.11/site-packages (from botocore<1.30.0,>=1.29.109->boto3) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.8.0->pyhealth) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.8.0->pyhealth) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.8.0->pyhealth) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.8.0->pyhealth) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->pyhealth) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.8.0->pyhealth) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install the required dependencies\n",
    "%pip install boto3 python-dotenv pandas pyhealth matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8383fc65",
   "metadata": {},
   "source": [
    "### Get MIMIC-3 Data\n",
    "Attempt to load MIMIC-3 data out of S3 if the relevant CSV files are not already in the mimic3 folder at the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9753cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ADMISSIONS.csv...\n",
      "Found PATIENTS.csv...\n",
      "Found DIAGNOSES_ICD.csv...\n",
      "Found PROCEDURES_ICD.csv...\n",
      "Found TRANSFERS.csv...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils import copy_file_from_s3\n",
    "\n",
    "data_folder = \"mimic3\"\n",
    "required_files = [\"ADMISSIONS.csv\", \"PATIENTS.csv\", \"DIAGNOSES_ICD.csv\", \"PROCEDURES_ICD.csv\", \"TRANSFERS.csv\"]\n",
    "\n",
    "for i, fn in enumerate(required_files):\n",
    "  if not os.path.exists(f\"{data_folder}/{fn}\"):\n",
    "    print(f\"Cannot find {fn} in {data_folder}, trying to download from S3...\")\n",
    "    copy_file_from_s3(fn, data_folder)\n",
    "  else:\n",
    "    print(f\"Found {fn}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9202ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset.patients: patient_id -> <Patient>\n",
      "\n",
      "<Patient>\n",
      "    - visits: visit_id -> <Visit> \n",
      "    - other patient-level info\n",
      "    \n",
      "    <Visit>\n",
      "        - event_list_dict: table_name -> List[Event]\n",
      "        - other visit-level info\n",
      "    \n",
      "        <Event>\n",
      "            - code: str\n",
      "            - other event-level info\n",
      "\n",
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 46520\n",
      "\t- Number of visits: 58976\n",
      "\t- Number of visits per patient: 1.2678\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.0711\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 46520\\n\\t- Number of visits: 58976\\n\\t- Number of visits per patient: 1.2678\\n\\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0711\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\"./mimic3/\", [\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"]) #pyhealth does not support mapping ICD-9 to ICD-10 codes.\n",
    "\n",
    "mimic3_ds.info()\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0df3a35f",
   "metadata": {},
   "source": [
    "## Sequencing EMR: Creating Sentences representing patient episodes\n",
    "Per Deepr, an EMR must be translated into a sentence for use downstream the model. An EMR is a sequence of time-stamped visit episodes. Each episode involves a series of diagnoses and treatments, called a phrase. Each phrase is separated by a time interval equal to `(0–1], (1–3], (3–6], (6–12], and 12+` or `TRANSFER`, with the latter indicating a transfer between care providers (separate departments within the same hospital or between hospitals.) Infrequent words are coded with `RAREWORD`, which indicates the word has appeared <100 times. Per the Deepr paper, an example sentence looks as follows:\n",
    "\n",
    "```\n",
    "1910 Z83 911 1008 D12 K31 1-3m R94 RAREWORD H53 Y83 M62 Y92 E87 T81 RAREWORD RAREWORD 1893 D12 S14 738 1910 1916 Z83 0-1m T91 RAREWORD Y83 Y92 K91 M10 E86 6-12m K31 1008 1910 Z13 Z83.\n",
    "```\n",
    "\n",
    "Note: In the sentence above, diagnoses are in ICD-10 format (a character followed by digits) and procedures are in digits. \n",
    "\n",
    "The MIMIC-3 dataset provides ICD-9 codes, and these will be used, but the level-3 variant of them for consistency with the original paper. It can also be noted that the encounter and discharge datetimes for visits are between the years 2100-2200 in order to deidentify patients, however, the time interval between visits is indeed preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852caebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find rare words (diagnoses and procedures with counts of less than 100)\n",
    "'''\n",
    "word_cnts = {}\n",
    "\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "  words = []\n",
    "\n",
    "  for _, v in p.visits.items():\n",
    "    for e in v.get_event_list('DIAGNOSES_ICD'):\n",
    "      words.append(e.code)\n",
    "\n",
    "    for e in v.get_event_list('PROCEDURES_ICD'):\n",
    "      words.append(e.code)\n",
    "\n",
    "  for word in words:\n",
    "    # If the word is already in the dictionary, increment the count\n",
    "    if word in word_cnts:\n",
    "        word_cnts[word] += 1\n",
    "    # Otherwise, add the word to the dictionary with a count of 1\n",
    "    else:\n",
    "        word_cnts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d19407",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Append to pyhealth's record of visits so that ADMISSION_TYPE and ADMISSION_LOCATION data are available. The former is used later in the model training\n",
    "to target against readmission (by looking at non-elective admits) and the latter is used to discern intra/inter hospital transfers, which is needed to build \n",
    "the sentences already described (i.e. to help build sentences using the TRANSFER keyword.)\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "admissions_df = pd.read_csv(\"./mimic3/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.set_index(\"HADM_ID\")\n",
    "\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "  # Sort patient visits by encounter_time\n",
    "  for i, v in enumerate(p.visits.items(),):\n",
    "    res = admissions_df.loc[int(v[1].visit_id)]\n",
    "\n",
    "    v[1].attr_dict[\"ADMISSION_TYPE\"] = res[\"ADMISSION_TYPE\"]\n",
    "    v[1].attr_dict[\"ADMISSION_LOCATION\"] = res[\"ADMISSION_LOCATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc89c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import timedelta_to_interval\n",
    "import random\n",
    "import json\n",
    "\n",
    "'''\n",
    "Translate EMRs into sentences outlined by the paper. A sentence consists of phrases, which are randomly shuffled diagnosis and procedure codes, separated by the \n",
    "time interval between visits, if the time interval exists. Sentences should have 100 words max.\n",
    "\n",
    "While looping over each patient:\n",
    "  1. Sort visits by encounter_time\n",
    "  2. Find the time interval between each visit and generate its relevant string word\n",
    "  3. Build arrays of diagnosis and procedure codes for each visit, replacing ICD-10 codes with less than 100 usages with RAREWORD\n",
    "  4. Randomly shuffle each array of diagnosis and procedure codes, then append the time interval string if available. This represents a phrase.\n",
    "    Concat each phrase to an array, which will be concatenated to form the final sentence. If the concatenation would form a sentence longer\n",
    "    than 100 words, min(100, words(sentence)) is adhered to.\n",
    "'''\n",
    "sentences = []\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "\n",
    "  # Sort patient visits by encounter_time\n",
    "  sorted_visits = sorted(p.visits.items(), key=lambda v: v[1].encounter_time) # sort by encounter time in order to guage interval between visits\n",
    "\n",
    "  # Generate timestamps in between visits\n",
    "  discharge_times = [visit[1].discharge_time for visit in sorted_visits[:-1]]\n",
    "  encounter_times = [visit[1].encounter_time for visit in sorted_visits[1:]]\n",
    "\n",
    "  time_intervals = [\n",
    "      t2 - t1\n",
    "      for t1, t2 in zip(discharge_times, encounter_times)\n",
    "  ]\n",
    "\n",
    "  # Convert timestamps to month intervals as specified in paper\n",
    "  time_interval_strs = timedelta_to_interval(time_intervals)\n",
    "\n",
    "  # event_diagnoses_ls = (visit, diagnoses_codes)\n",
    "  event_diagnoses_ls = []\n",
    "\n",
    "  # event_procedures_ls = (visit, procedure_codes)\n",
    "  event_procedures_ls = []\n",
    "\n",
    "  # Helper function to create arrays with RAREWORD using list comprehension\n",
    "  def handle_event(event_list, word_cnts):\n",
    "      return [\"RAREWORD\" if e.code in word_cnts and word_cnts[e.code] < 100 else e.code for e in event_list]\n",
    "\n",
    "  # build arrays of diagnoses and procedures on a visit level, add to event_diagnoses_ls or event_procedures_ls\n",
    "  for _, v in sorted_visits:\n",
    "      visit_diagnoses = handle_event(v.get_event_list('DIAGNOSES_ICD'), word_cnts)\n",
    "      event_diagnoses_ls.append(visit_diagnoses)\n",
    "\n",
    "      visit_procedures = handle_event(v.get_event_list('PROCEDURES_ICD'), word_cnts)\n",
    "      event_procedures_ls.append(visit_procedures)\n",
    "\n",
    "\n",
    "  # Randomly shuffle diagnosis and procedure codes and append a time interval after, if available. Ensure the output sentence will not be more than 100 words.\n",
    "  arrs = []\n",
    "  word_cnt = 0\n",
    "  for i, vd in enumerate(event_diagnoses_ls):\n",
    "      arr = vd + event_procedures_ls[i]\n",
    "      random.shuffle(arr)\n",
    "      if i < len(time_interval_strs):\n",
    "          arr.append(time_interval_strs[i])\n",
    "\n",
    "      new_word_cnt = word_cnt + len(arr)\n",
    "\n",
    "      if new_word_cnt > 100:\n",
    "          # Calculate the number of elements needed to reach exactly 100 words\n",
    "          elements_needed = 100 - word_cnt\n",
    "\n",
    "          # Take a subset of arr to make new_word_cnt equal 100\n",
    "          arr = arr[:elements_needed]\n",
    "          arrs.append(arr)\n",
    "          break\n",
    "\n",
    "      arrs.append(arr)\n",
    "      word_cnt = new_word_cnt\n",
    "\n",
    "  # Combine all codes and time interval to create a phrase, representing a visit\n",
    "  phrases = [\" \".join(arr) for arr in arrs]\n",
    "\n",
    "  # Combine all phrases to create a sentence, representing a sequence as outlined by the paper\n",
    "  sentence = \" \".join(phrases)\n",
    "  sentences.append(sentence)\n",
    "\n",
    "# output to json file\n",
    "output_dir = \"data\"\n",
    "output_filename = \"sentences.json\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, output_filename), \"w\") as json_file:\n",
    "  json.dump(sentences, json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9d6957a",
   "metadata": {},
   "source": [
    "### Test that output sentences satisfy the following conditions:\n",
    "- There is a sentence for each patient\n",
    "- Each sentence is capped to max 100 words\n",
    "- Multi visit patients have visits separated by a timestamp\n",
    "- Words should not exist in their ICD-10 form if used less than 100 times (should be replaced with RAREWORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b42736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# There should be one sentence per patient\n",
    "num_patients = len(mimic3_ds.patients)\n",
    "num_sentences = len(sentences)\n",
    "assert(num_patients == num_sentences)\n",
    "\n",
    "# There should be max 100 words per sentence\n",
    "word_lengths = map(lambda s: len(s.split()), sentences)\n",
    "assert(max(list(word_lengths)) <= 100)\n",
    "\n",
    "# There should be no word in any of the sentences that is present less than 100 times\n",
    "rarewords = [word for word, count in word_cnts.items() if count < 100]\n",
    "for sentence in sentences:\n",
    "  words_of_sentence = sentence.split()\n",
    "  rareword_violations = list(filter(lambda w: w in word_cnts and word_cnts[w] < 100, words_of_sentence))\n",
    "  assert(len(rareword_violations) == 0)\n",
    "\n",
    "\n",
    "# Patients with multiple visits should have timestamps separating their visits i.e. 1-3m or 12+m #TODO: Add TRANSFER to regex\n",
    "pattern = re.compile(r\"[-+]\")\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "    if len(p.visits) > 1:\n",
    "        if not pattern.search(sentences[i]):\n",
    "            print(f\"Failed assertion for sentences[{i}]: '{sentences[i]}'\")\n",
    "            assert(False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f873178",
   "metadata": {},
   "source": [
    "## Training Word2Vec\n",
    "\n",
    "Word2Vec is a machine learning algorithm used to generate word embeddings that capture the semantic meaning of words. In the context of medical data analysis, Word2Vec can be used to analyze the relationship between diagnoses and treatments given an underlying disease. Instead of text, however, we can use ICD9 codes as words to create the embeddings. By training the model on a large corpus of medical records, we can identify relationships between different diagnoses and treatments, and group them into clusters denoting comorbidities. These clusters can help identify patterns in the data that may be useful for predicting disease outcomes and developing targeted treatment plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3e79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset, tok2id, wsize=3):\n",
    "        self.wsize = wsize\n",
    "        self.tok2id = tok2id\n",
    "\n",
    "        self.dataset = [\n",
    "            ctx\n",
    "            for sentence in dataset for ctx in self.get_contexts(sentence)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def get_contexts(self, sentence):\n",
    "        moving_window = []\n",
    "\n",
    "        for word_ix, word in enumerate(sentence):\n",
    "            target = self.tok2id[word]\n",
    "            window = [\n",
    "                word_ix + win_ix\n",
    "                for win_ix in range(-self.wsize, self.wsize + 1)\n",
    "                if (\n",
    "                    word_ix + win_ix >= 0\n",
    "                    and word_ix + win_ix < len(sentence)\n",
    "                    and win_ix != 0\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            moving_window += [\n",
    "                (target, self.tok2id[sentence[win_word]]) for win_word in window\n",
    "            ]\n",
    "\n",
    "        return moving_window\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.prediction = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.embedding(input)\n",
    "        logits = self.prediction(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "dataset = [sentence.split(\" \") for sentence in sentences]\n",
    "vocab = set([word for sentence in dataset for word in sentence])\n",
    "\n",
    "id2tok = dict(enumerate(vocab))\n",
    "tok2id = {token: id for id, token in id2tok.items()}\n",
    "\n",
    "word2vec_dataloader = DataLoader(\n",
    "    Word2VecDataset(dataset, tok2id),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "word2vec_model = Word2Vec(len(vocab), embedding_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "LR = 3e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(word2vec_model.parameters(), lr=LR)\n",
    "progress_bar = tqdm(range(EPOCHS * len(word2vec_dataloader)))\n",
    "\n",
    "running_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for center, context in word2vec_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = word2vec_model(input=context)\n",
    "        loss = loss_fn(logits, center)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    epoch_loss /= len(word2vec_dataloader)\n",
    "    running_loss.append(epoch_loss)\n",
    "\n",
    "plt.plot(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec_model.state_dict()\n",
    "torch.save(weights, os.path.join(output_dir, \"word2vec.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8ef13fa",
   "metadata": {},
   "source": [
    "## Convolutional Network\n",
    "Now that Word2Vec has created embeddings for the sentences we generated in the first step, we are ready to pass the embeddings through a convolutional layer, followed by max pooling.\n",
    "\n",
    "First, we will generate the train and val loaders. This will first require us to create the validation set. In Deepr, we try to predict the likelihood of unplanned re-admission after both 3 and 6 months. Unplanned re-admission is coded in the database as an unplanned or emergency with the status not equal to \"elective\" or a transfer from another hospital.\n",
    "\n",
    "Next, we will define a model containing a convolutional network, ReLU and max pooling layer.\n",
    "\n",
    "Deepr reported the following parameters were determined to be optimal for the model:\n",
    "\n",
    "`m = 100, d = 1, motif size = 3, 4, and 5, n = 100 number of epochs = 10, minibatch size = 64, and l2 regularization λ = 1.0.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60dd3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create validation dataset, train and val loaders for training Deepr.\n",
    "'''\n",
    "\n",
    "val_readmission_time_days = 180 # The threshold for flagging unplanned readmission for validation set\n",
    "risk_group = []\n",
    "remaining_group = [] #patients not put into the risk group\n",
    "empty_sentence_cnt = 0\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "\n",
    "  # Sort patient visits by encounter_time\n",
    "  sorted_visits = sorted(p.visits.items(), key=lambda v: v[1].encounter_time) # sort by encounter time in order to guage interval between visits\n",
    "\n",
    "  # Generate timestamps in between visits\n",
    "  discharge_times = [visit[1].discharge_time for visit in sorted_visits[:-1]]\n",
    "  encounter_times = [visit[1].encounter_time for visit in sorted_visits[1:]]\n",
    "\n",
    "  time_intervals = [\n",
    "      t2 - t1\n",
    "      for t1, t2 in zip(discharge_times, encounter_times)\n",
    "  ]\n",
    "\n",
    "  visit_types = [visit[1].attr_dict[\"ADMISSION_TYPE\"] for visit in sorted_visits[1:]]\n",
    "\n",
    "  unplanned_readmissions = [\n",
    "      i_vt\n",
    "      for i_vt, (interval, visit_type) in enumerate(zip(time_intervals, visit_types))\n",
    "      if interval.days <= val_readmission_time_days and visit_type == \"EMERGENCY\"\n",
    "  ]\n",
    "\n",
    "  if sentences[i]:\n",
    "    if unplanned_readmissions:\n",
    "      risk_group.append(sentences[i])\n",
    "    else: \n",
    "      remaining_group.append(sentences[i])\n",
    "  else:\n",
    "    empty_sentence_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a59f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk group patients should be separate from the candidate group, and sum to the length of the total dataset\n",
    "assert(len(mimic3_ds.patients.values()) == len(risk_group) + len(remaining_group) + empty_sentence_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c642e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "'''\n",
    "Dataset for Deepr model. __getitem__ returns the word indices for a given sentence. \n",
    "DeeprModel will then look up the word embeddings by the index in the embedding table.\n",
    "'''\n",
    "\n",
    "class DeeprDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, word2index):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.word2index = word2index\n",
    "\n",
    "    def get_indices(self, sentence):\n",
    "        return torch.tensor([self.word2index[word] for word in sentence.split()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.get_indices(self.x_data[idx])\n",
    "        y = self.y_data[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4dc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "'''\n",
    "The sentences in each batch will likely not all be the same size, and thus we need to pad the sentences. \n",
    "DeeprDataset returns the indicies of the words in each sentence. We will padd these vectors of indicies with 0s.\n",
    "It is important later on ensure that index 0 of the embedding table is a torch.zero(embedding_dim), as these are just placeholder values,\n",
    "and to mask them out later on.\n",
    "'''\n",
    "\n",
    "def pad_collate(batch):\n",
    "    x_data, y_data = zip(*batch)\n",
    "    x_data = list(x_data)\n",
    "\n",
    "    # TODO: Receive max_len as an argument\n",
    "    padded_fst = torch.zeros(100)  # max_len\n",
    "    padded_fst[: x_data[0].shape[0]] = x_data[0]\n",
    "    x_data[0] = padded_fst\n",
    "\n",
    "    x_data_padded = pad_sequence(x_data, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Create binary mask\n",
    "    mask = (x_data_padded != 0).float()\n",
    "\n",
    "    # Convert from tuple to tensor\n",
    "    y_data_tensor = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "    return x_data_padded.long(), y_data_tensor, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d0845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "'''\n",
    "Generate the training and validation sets. The training data is a 1-1 mixture of risk and control patients, where x \n",
    "is the sentence of the patient, and y is the corresponding binary label of readmission (1=readmit, 0=non-readmit)\n",
    "\n",
    "We also ensure that index 0 of the embedding table is associated with the word \"<pad>\", so that the padding index has an associated word, \n",
    "as it was not learned in Word2Vec, but will be used in the Deepr model later on.\n",
    "'''\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "control_group = random.sample(remaining_group, len(risk_group))\n",
    "control_labels = [0] * len(control_group)\n",
    "control_zip = list(zip(control_group, control_labels))\n",
    "\n",
    "risk_labels = [1] * len(risk_group)\n",
    "risk_zip = list(zip(risk_group, risk_labels))\n",
    "\n",
    "all_zip = list(control_zip + risk_zip)\n",
    "\n",
    "x, y = zip(*all_zip)\n",
    "\n",
    "# Shift all elements up, and add the padding word at index 0\n",
    "word2index = {word: idx + 1 for idx, word in enumerate(tok2id)}\n",
    "word2index[\"<pad>\"] = 0\n",
    "\n",
    "train_dataset, cv_dataset, val_dataset = random_split(DeeprDataset(x, y, word2index), [0.64, 0.16, 0.2])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "cv_dataloader = DataLoader(cv_dataset, batch_size=64, shuffle=False, collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e270c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "DeeprModel does the following things:\n",
    "\n",
    "1. During initialization, updates the embedding table learned from Word2Vec to include the embedding for padding.\n",
    "2. Pretrains an embedding layer with the weights from Word2Vec.\n",
    "3. Initializes the model with hyperparameters from the Deepr paper, mentioned at the beginning of this section.\n",
    "4. Passes the embeddings through convolutional, max pooling and a linear classifier.\n",
    "'''\n",
    "\n",
    "class DeeprModel(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, num_classes, max_len, vocab_size, embedding=None, with_cnn=True, with_attn=True):\n",
    "        super(DeeprModel, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.with_cnn = with_cnn\n",
    "        self.with_attn = with_attn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # Layers\n",
    "        if embedding:\n",
    "            self.embedding = embedding\n",
    "            in_channels = self.embedding.weight.shape[1]\n",
    "        else:\n",
    "            self.embedding = None\n",
    "            in_channels = vocab_size\n",
    "\n",
    "        if with_cnn:\n",
    "            self.conv = nn.Conv1d(in_channels, num_filters, kernel_size, padding='same')\n",
    "            self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "            out_channels = num_filters\n",
    "        else:\n",
    "            self.rnn = nn.GRU(in_channels, num_filters, batch_first=True, bidirectional=True)\n",
    "            self.attn = nn.Linear(2 * num_filters * max_len, max_len)\n",
    "            out_channels = 2 * num_filters\n",
    "\n",
    "        self.fc = nn.Linear(out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        if self.embedding:\n",
    "            x = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        else:\n",
    "            x = nn.functional.one_hot(x, num_classes=self.vocab_size).float()  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        if self.with_cnn:\n",
    "            x = x.transpose(1, 2)  # conv requires input_channels to be dim 1. Shape: (batch_size, embedding_dim, sequence_length)\n",
    "            x = F.relu(self.conv(x))  # Shape: (batch_size, num_filters, sequence_length)\n",
    "            x = x * mask.unsqueeze(1)  # Apply mask\n",
    "            relu_output = x.detach().cpu().numpy() # used in motif detection analysis later; not part of training/validation\n",
    "            x = self.pool(x).squeeze(2)  # Shape: (batch_size, num_filters)\n",
    "        else:\n",
    "            sequence_lengths = mask.sum(dim=1).long()\n",
    "            x_packed = nn.utils.rnn.pack_padded_sequence(x, sequence_lengths, batch_first=True, enforce_sorted=False)\n",
    "            output, h_n = self.rnn(x_packed)\n",
    "\n",
    "            if self.with_attn:\n",
    "                # Force the output of the RNN to have shape: (batch_size, max_len, 2 * num_filters)\n",
    "                output = nn.utils.rnn.unpack_sequence(output)\n",
    "                padded_sen = torch.zeros(self.max_len, 2 * self.num_filters)\n",
    "                padded_sen[: output[0].shape[0]] = output[0]\n",
    "                output[0] = padded_sen\n",
    "                padded_output = pad_sequence(output, batch_first=True)\n",
    "\n",
    "                # Calculate masked attention scores\n",
    "                attn_weights = self.attn(padded_output.view(padded_output.shape[0], padded_output.shape[1] * padded_output.shape[2]))\n",
    "                attn_weights[mask == 0] = -1e9\n",
    "                attn_score = F.softmax(attn_weights, dim=1)[:, :, None]\n",
    "\n",
    "                # Apply attention\n",
    "                x = (attn_score * padded_output).sum(dim=1)\n",
    "            else:\n",
    "                x = torch.cat((h_n[-1], h_n[-2]), dim=1)  # Shape: (batch_size, 2 * num_filters)\n",
    "\n",
    "            relu_output = None\n",
    "\n",
    "        x = self.fc(x)  # Shape: (batch_size, num_classes)\n",
    "        return x, relu_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09cb756e",
   "metadata": {},
   "source": [
    "## Training and validating DeeprModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4363c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is a helper function to take the output of the filter responses (ReLU(CNN(x)) in Deepr's forward method) and map them back to the original sentences. \n",
    "Strong filter responses are indicative that the motif within the convolutional window contributes towards the model predicting unplanned hospital readmission.\n",
    "'''\n",
    "\n",
    "# Remaps tok2id to include the padding index used in Deepr.\n",
    "index2WordValidation = {idx+1: word for idx, word in enumerate(tok2id)}\n",
    "index2WordValidation[0] = '<pad>'\n",
    "\n",
    "# Helper function\n",
    "def normalize_array(arr, max_val):\n",
    "    min_val = 0\n",
    "    normalized = [1 + (x - min_val) * (2 - 1) / (max_val - min_val) for x in arr]\n",
    "    return normalized\n",
    "\n",
    "def getMotifs(batch_x, relu_output, probabilities, filter_num=0):\n",
    "    '''\n",
    "    Gets the top 10 most likely readmit and 10 most unlikely readmits for the given batch. Used to analyze the motifs most strongly contributing to readmission.\n",
    "\n",
    "    batch_x contains the batch in the validation loop (shape: batch_size, padded_sentence_length)\n",
    "    relu_output contains the filter responses from Deepr (shape: batch_size, num_filters, padded_sentence_length)\n",
    "    probabilities contains the probabilities of the sentences being in the positive class (positive readmit probability). (shape: batch_size)\n",
    "    filter_num is a 0 indexed number corresponding to which filter from the CNN the motifs should be pulled from.\n",
    "    '''\n",
    "\n",
    "    output_list = []\n",
    "    for i in range(batch_x.size(0)):\n",
    "      # Remap indices from tensors in batch back to their original sentences\n",
    "      word_list = [index2WordValidation[idx.item()] for idx in batch_x[i]]\n",
    "      relu_values = relu_output[i][filter_num].tolist()\n",
    "      is_positive = probabilities[i] > 0.5\n",
    "\n",
    "      instance_data = {\n",
    "          \"sentence\": \" \".join(word_list),\n",
    "          \"is_positive\": bool(is_positive),\n",
    "          \"is_positive_score\": probabilities[i].item(),\n",
    "          \"raw_motif_values\": relu_values\n",
    "      }\n",
    "\n",
    "      output_list.append(instance_data)\n",
    "\n",
    "    return output_list\n",
    "\n",
    "def normalize_motifs(motifs, max_motif_val):\n",
    "  for i in range(len(motifs)):\n",
    "    normalized_motifs = normalize_array(motifs[i][\"raw_motif_values\"], max_motif_val)\n",
    "    motifs[i][\"normalized_motif_values\"] = normalized_motifs\n",
    "  return motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f8bf065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6993, AUC: 0.3886\n",
      "Epoch 2, Loss: 0.6951, AUC: 0.4521\n",
      "Epoch 3, Loss: 0.6932, AUC: 0.5165\n",
      "Epoch 4, Loss: 0.6921, AUC: 0.5755\n",
      "Epoch 5, Loss: 0.6916, AUC: 0.6268\n",
      "Epoch 6, Loss: 0.6914, AUC: 0.6674\n",
      "Epoch 7, Loss: 0.6914, AUC: 0.7003\n",
      "Epoch 8, Loss: 0.6914, AUC: 0.7267\n",
      "Epoch 9, Loss: 0.6916, AUC: 0.7475\n",
      "Epoch 10, Loss: 0.6917, AUC: 0.7643\n",
      "Epoch 11, Loss: 0.6919, AUC: 0.7781\n",
      "Epoch 12, Loss: 0.6921, AUC: 0.7895\n",
      "Epoch 13, Loss: 0.6922, AUC: 0.7987\n",
      "Epoch 14, Loss: 0.6924, AUC: 0.8062\n",
      "Epoch 15, Loss: 0.6925, AUC: 0.8126\n",
      "Epoch 16, Loss: 0.6926, AUC: 0.8178\n",
      "Epoch 17, Loss: 0.6927, AUC: 0.8227\n",
      "Epoch 18, Loss: 0.6928, AUC: 0.8268\n",
      "Epoch 19, Loss: 0.6929, AUC: 0.8302\n",
      "Epoch 20, Loss: 0.6929, AUC: 0.8330\n",
      "Epoch 21, Loss: 0.6930, AUC: 0.8353\n",
      "Epoch 22, Loss: 0.6930, AUC: 0.8372\n",
      "Epoch 23, Loss: 0.6930, AUC: 0.8393\n",
      "Epoch 24, Loss: 0.6930, AUC: 0.8410\n",
      "Epoch 25, Loss: 0.6931, AUC: 0.8423\n",
      "Epoch 26, Loss: 0.6931, AUC: 0.8429\n",
      "Epoch 27, Loss: 0.6931, AUC: 0.8431\n",
      "Epoch 28, Loss: 0.6931, AUC: 0.8432\n",
      "Epoch 29, Loss: 0.6931, AUC: 0.8432\n",
      "Epoch 30, Loss: 0.6931, AUC: 0.8433\n",
      "Epoch 31, Loss: 0.6931, AUC: 0.8435\n",
      "Epoch 32, Loss: 0.6931, AUC: 0.8441\n",
      "Epoch 33, Loss: 0.6931, AUC: 0.8453\n",
      "Epoch 34, Loss: 0.6932, AUC: 0.8463\n",
      "Epoch 35, Loss: 0.6932, AUC: 0.8471\n",
      "Epoch 36, Loss: 0.6932, AUC: 0.8473\n",
      "Epoch 37, Loss: 0.6932, AUC: 0.8474\n",
      "Epoch 38, Loss: 0.6932, AUC: 0.8473\n",
      "Epoch 39, Loss: 0.6932, AUC: 0.8476\n",
      "Epoch 40, Loss: 0.6932, AUC: 0.8482\n",
      "Epoch 41, Loss: 0.6932, AUC: 0.8488\n",
      "Epoch 42, Loss: 0.6932, AUC: 0.8494\n",
      "Epoch 43, Loss: 0.6932, AUC: 0.8498\n",
      "Epoch 44, Loss: 0.6932, AUC: 0.8497\n",
      "Epoch 45, Loss: 0.6932, AUC: 0.8498\n",
      "Epoch 46, Loss: 0.6932, AUC: 0.8499\n",
      "Epoch 47, Loss: 0.6932, AUC: 0.8491\n",
      "Epoch 48, Loss: 0.6932, AUC: 0.8483\n",
      "Epoch 49, Loss: 0.6932, AUC: 0.8487\n",
      "Test Loss: 0.6931, Test AUC: 0.8402\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "\n",
    "\n",
    "'''\n",
    "We first initialize the model's hyperparameters. Then, we load the saved weights from Word2Vec into a new word2vec_model instance. \n",
    "We initialize a DeeprModel instance, CrossEntropyLoss function, and SGD optimizer, the later two being specified in the Deepr paper.\n",
    "In the training loop, we compare the logits computed in the forward method of the model against the labels we computed in other steps,\n",
    "compute the loss, back propogate and update weights.\n",
    "\n",
    "In validating the data, we compute validation loss and AUC.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_data, cv_data):\n",
    "    model.train()\n",
    "\n",
    "    for batch_x, batch_y, mask in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(batch_x, mask)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for batch_x, batch_y, mask in cv_data:\n",
    "            logits, _ = model(batch_x, mask)\n",
    "\n",
    "            loss = criterion(logits, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get probabilities for the positive class\n",
    "            probabilities = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            all_logits.extend(probabilities)\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(cv_data)\n",
    "    auc = roc_auc_score(all_labels, all_logits)\n",
    "\n",
    "    return {'state_dict': model.state_dict(), 'loss': val_loss, 'auc': auc}\n",
    "\n",
    "\n",
    "def get_min_std(models):\n",
    "    aucs = torch.tensor([model['auc'] for model in models])\n",
    "\n",
    "    return torch.min(aucs), torch.std(aucs)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "l2_reg = 1.0\n",
    "\n",
    "# Initialize the model\n",
    "seq_max_len = 100\n",
    "num_filters = 64\n",
    "filter_size = 3\n",
    "num_classes = 2  # For binary classification\n",
    "embedding_dim = 100\n",
    "with_cnn = True\n",
    "with_attn = True\n",
    "\n",
    "\n",
    "# Load weights from .pt file\n",
    "vocab_size = len(vocab)\n",
    "word2vec_model = Word2Vec(vocab_size, embedding_dim)\n",
    "weights_path = os.path.join(output_dir, \"word2vec_time.pt\")\n",
    "saved_weights = torch.load(weights_path)\n",
    "\n",
    "word2vec_model.load_state_dict(saved_weights)\n",
    "\n",
    "word_embeddings_dict = {\n",
    "    word: word2vec_model.embedding(torch.tensor(tok2id[word])).detach().numpy()\n",
    "    for word in tok2id\n",
    "}\n",
    "\n",
    "# Prepare the embedding matrix; this takes the ouput of word2vec but inserts \"<pad>\" string, assigning an empty embedding to it\n",
    "embedding_matrix = torch.zeros(len(word_embeddings_dict) + 1, embedding_dim)\n",
    "for word, index in word2index.items():\n",
    "    if word != \"<pad>\":\n",
    "        # embedding_matrix[index] = torch.rand(1, embedding_dim)\n",
    "        embedding_matrix[index] = torch.tensor(word_embeddings_dict[word])\n",
    "\n",
    "deepr_embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
    "model = DeeprModel(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    num_classes,\n",
    "    seq_max_len,\n",
    "    len(word_embeddings_dict) + 1,\n",
    "    embedding=deepr_embedding,\n",
    "    with_cnn=with_cnn,\n",
    "    with_attn=with_attn\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
    "\n",
    "potential_models = deque()\n",
    "\n",
    "for epoch in range(10):\n",
    "    new_model = train_model(model, criterion, optimizer, train_dataloader, cv_dataloader)\n",
    "\n",
    "    potential_models.append(new_model)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {new_model['loss']:.4f}, AUC: {new_model['auc']:.4f}\")\n",
    "\n",
    "min, std = get_min_std(potential_models)\n",
    "threshold = min - std\n",
    "\n",
    "for epoch in itertools.count(start=11):\n",
    "    potential_models.popleft()\n",
    "\n",
    "    new_model = train_model(model, criterion, optimizer, train_dataloader, cv_dataloader)\n",
    "\n",
    "    if new_model['auc'] < threshold:\n",
    "        break\n",
    "\n",
    "    potential_models.append(new_model)\n",
    "\n",
    "    min, std = get_min_std(potential_models)\n",
    "\n",
    "    if std < 1e-4:\n",
    "        break\n",
    "\n",
    "    threshold = min - std\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {new_model['loss']:.4f}, AUC: {new_model['auc']:.4f}\")\n",
    "\n",
    "min_ix = int(torch.tensor([m['auc'] for m in potential_models]).argmin().item())\n",
    "model.load_state_dict(potential_models[min_ix]['state_dict'])\n",
    "\n",
    "max_motif_val = 0.0\n",
    "motifs_data = []\n",
    "\n",
    "val_loss = 0\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "# Validation\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for batch_x, batch_y, mask in val_dataloader:\n",
    "        logits, relu_output = model(batch_x, mask)\n",
    "        \n",
    "        loss = criterion(logits, batch_y)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Get probabilities for the positive class\n",
    "        probabilities = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "        if with_cnn:\n",
    "            # Output motifs data for each sentence in the batch\n",
    "            batch_motifs = getMotifs(batch_x, relu_output, probabilities)\n",
    "            motifs_data.extend(batch_motifs)\n",
    "\n",
    "            # find max value of the batch and update the max motif value across the validation set\n",
    "            max_batch_motif_val = np.max(relu_output)\n",
    "            max_motif_val = max(max_batch_motif_val, max_motif_val)\n",
    "\n",
    "        all_logits.extend(probabilities)\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "val_loss /= len(val_dataloader)\n",
    "auc = roc_auc_score(all_labels, all_logits)\n",
    "\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test AUC: {auc:.4f}\")\n",
    "\n",
    "if with_cnn:\n",
    "  # Normalize the motifs across the validation set\n",
    "  motifs_data_normalized = normalize_motifs(motifs_data, max_motif_val)\n",
    "  # sort the motifs from most at risk of unplanned readmit to least\n",
    "  motifs_data_normalized = sorted(motifs_data_normalized, key=lambda x: x['is_positive_score'], reverse=True)\n",
    "\n",
    "  # Save the sentence level motifs to a JSON file\n",
    "  with open(os.path.join(output_dir, \"motifs_data.json\"), \"w\") as outfile:\n",
    "      json.dump(motifs_data_normalized, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
