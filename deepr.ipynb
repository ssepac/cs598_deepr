{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0307e9",
   "metadata": {},
   "source": [
    "# CS598 Deep Learning for Healthcare Final Project\n",
    "## Reproduction of Deepr: A Convolutional Net for Medical Records\n",
    "### Juan Alvarez Martinez, Shane Sepac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec06a5b",
   "metadata": {},
   "source": [
    "## Reproducibility Summary\n",
    "We implemented *Deepr: A Convolutional Net for Medical Records* and achieved an AUC in the model validation consistent and if not better than the original paper reported. The MIMIC-3 dataset was used for testing against the Deepr paper. The python library *Pyhealth* was used to ingest the MIMIC-3 dataset in order to leverage patient, visit and event level data. These data were mapped into \"sentences,\" as referenced in the paper, which were formed by capturing the ICD-9 diagnoses and procedure codes for events within a given visit into \"phrases\", and each phrase concatenated between a timestamp indicating duration between patient visits. After generating sentences according to every patient's EMR records, we generated word embeddings for each sentence via Word2Vec (skip-gram), which was leveraged to help capture the context of surrounding words. The embeddings were pushed through our Deepr model implementation, which consists of convolutional, relu, max pooling and linear classification layers, which produced a binary classification of unplanned readmission likelihood. The data used to train the model was a 1-1 mixture of unplanned readmitted and non-readmitted patients. During validation, we achieved results consistent with the Deepr paper, with an AUC of ~0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0ba21",
   "metadata": {},
   "source": [
    "## Load MIMIC-III Dataset. \n",
    "Several csv files are needed from the MIMIC-III dataset: ADMISSIONS, PATIENTS, DIAGNOSES_ICD, and PROCEDURES_ICD. These files can be loaded automatically out of S3, or you can place them in `<project_root>/mimic3`. \n",
    "- If loading out of S3, ensure you have all environment variables from .env.sample copied and instantiated in a .env file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4700fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required dependencies\n",
    "%pip install boto3 python-dotenv pandas pyhealth matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383fc65",
   "metadata": {},
   "source": [
    "### Get MIMIC-3 Data\n",
    "Attempt to load MIMIC-3 data out of S3 if the relevant CSV files are not already in the mimic3 folder at the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import copy_file_from_s3\n",
    "\n",
    "data_folder = \"mimic3\"\n",
    "required_files = [\"ADMISSIONS.csv\", \"PATIENTS.csv\", \"DIAGNOSES_ICD.csv\", \"PROCEDURES_ICD.csv\", \"TRANSFERS.csv\"]\n",
    "\n",
    "for i, fn in enumerate(required_files):\n",
    "  if not os.path.exists(f\"{data_folder}/{fn}\"):\n",
    "    print(f\"Cannot find {fn} in {data_folder}, trying to download from S3...\")\n",
    "    copy_file_from_s3(fn, data_folder)\n",
    "  else:\n",
    "    print(f\"Found {fn}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9202ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\"./mimic3/\", [\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"]) #pyhealth does not support mapping ICD-9 to ICD-10 codes.\n",
    "\n",
    "mimic3_ds.info()\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3a35f",
   "metadata": {},
   "source": [
    "## Sequencing EMR: Creating Sentences representing patient episodes\n",
    "Per Deepr, an EMR must be translated into a sentence for use downstream the model. An EMR is a sequence of time-stamped visit episodes. Each episode involves a series of diagnoses and treatments, called a phrase. Each phrase is separated by a time interval equal to `(0–1], (1–3], (3–6], (6–12], and 12+` or `TRANSFER`, with the latter indicating a transfer between care providers (separate departments within the same hospital or between hospitals.) Infrequent words are coded with `RAREWORD`, which indicates the word has appeared <100 times. Per the Deepr paper, an example sentence looks as follows:\n",
    "\n",
    "```\n",
    "1910 Z83 911 1008 D12 K31 1-3m R94 RAREWORD H53 Y83 M62 Y92 E87 T81 RAREWORD RAREWORD 1893 D12 S14 738 1910 1916 Z83 0-1m T91 RAREWORD Y83 Y92 K91 M10 E86 6-12m K31 1008 1910 Z13 Z83.\n",
    "```\n",
    "\n",
    "Note: In the sentence above, diagnoses are in ICD-10 format (a character followed by digits) and procedures are in digits. \n",
    "\n",
    "The MIMIC-3 dataset provides ICD-9 codes, and these will be used, but the level-3 variant of them for consistency with the original paper. It can also be noted that the encounter and discharge datetimes for visits are between the years 2100-2200 in order to deidentify patients, however, the time interval between visits is indeed preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852caebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find rare words (diagnoses and procedures with counts of less than 100)\n",
    "'''\n",
    "word_cnts = {}\n",
    "\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "  words = []\n",
    "\n",
    "  for _, v in p.visits.items():\n",
    "    for e in v.get_event_list('DIAGNOSES_ICD'):\n",
    "      words.append(e.code)\n",
    "\n",
    "    for e in v.get_event_list('PROCEDURES_ICD'):\n",
    "      words.append(e.code)\n",
    "\n",
    "  for word in words:\n",
    "    # If the word is already in the dictionary, increment the count\n",
    "    if word in word_cnts:\n",
    "        word_cnts[word] += 1\n",
    "    # Otherwise, add the word to the dictionary with a count of 1\n",
    "    else:\n",
    "        word_cnts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d19407",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Append to pyhealth's record of visits so that ADMISSION_TYPE and ADMISSION_LOCATION data are available. The former is used later in the model training\n",
    "to target against readmission (by looking at non-elective admits) and the latter is used to discern intra/inter hospital transfers, which is needed to build \n",
    "the sentences already described (i.e. to help build sentences using the TRANSFER keyword.)\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "admissions_df = pd.read_csv(\"./mimic3/ADMISSIONS.csv\")\n",
    "admissions_df = admissions_df.set_index(\"HADM_ID\")\n",
    "\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "  # Sort patient visits by encounter_time\n",
    "  for i, v in enumerate(p.visits.items(),):\n",
    "    res = admissions_df.loc[int(v[1].visit_id)]\n",
    "\n",
    "    v[1].attr_dict[\"ADMISSION_TYPE\"] = res[\"ADMISSION_TYPE\"]\n",
    "    v[1].attr_dict[\"ADMISSION_LOCATION\"] = res[\"ADMISSION_LOCATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import timedelta_to_interval\n",
    "import random\n",
    "import json\n",
    "\n",
    "'''\n",
    "Translate EMRs into sentences outlined by the paper. A sentence consists of phrases, which are randomly shuffled diagnosis and procedure codes, separated by the \n",
    "time interval between visits, if the time interval exists. Sentences should have 100 words max.\n",
    "\n",
    "While looping over each patient:\n",
    "  1. Sort visits by encounter_time\n",
    "  2. Find the time interval between each visit and generate its relevant string word\n",
    "  3. Build arrays of diagnosis and procedure codes for each visit, replacing ICD-10 codes with less than 100 usages with RAREWORD\n",
    "  4. Randomly shuffle each array of diagnosis and procedure codes, then append the time interval string if available. This represents a phrase.\n",
    "    Concat each phrase to an array, which will be concatenated to form the final sentence. If the concatenation would form a sentence longer\n",
    "    than 100 words, min(100, words(sentence)) is adhered to.\n",
    "'''\n",
    "sentences = []\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "\n",
    "  # Sort patient visits by encounter_time\n",
    "  sorted_visits = sorted(p.visits.items(), key=lambda v: v[1].encounter_time) # sort by encounter time in order to guage interval between visits\n",
    "\n",
    "  # Generate timestamps in between visits\n",
    "  discharge_times = [visit[1].discharge_time for visit in sorted_visits[:-1]]\n",
    "  encounter_times = [visit[1].encounter_time for visit in sorted_visits[1:]]\n",
    "\n",
    "  time_intervals = [\n",
    "      t2 - t1\n",
    "      for t1, t2 in zip(discharge_times, encounter_times)\n",
    "  ]\n",
    "\n",
    "  # Convert timestamps to month intervals as specified in paper\n",
    "  time_interval_strs = timedelta_to_interval(time_intervals)\n",
    "\n",
    "  # event_diagnoses_ls = (visit, diagnoses_codes)\n",
    "  event_diagnoses_ls = []\n",
    "\n",
    "  # event_procedures_ls = (visit, procedure_codes)\n",
    "  event_procedures_ls = []\n",
    "\n",
    "  # Helper function to create arrays with RAREWORD using list comprehension\n",
    "  def handle_event(event_list, word_cnts):\n",
    "      return [\"RAREWORD\" if e.code in word_cnts and word_cnts[e.code] < 100 else e.code for e in event_list]\n",
    "\n",
    "  # build arrays of diagnoses and procedures on a visit level, add to event_diagnoses_ls or event_procedures_ls\n",
    "  for _, v in sorted_visits:\n",
    "      visit_diagnoses = handle_event(v.get_event_list('DIAGNOSES_ICD'), word_cnts)\n",
    "      event_diagnoses_ls.append(visit_diagnoses)\n",
    "\n",
    "      visit_procedures = handle_event(v.get_event_list('PROCEDURES_ICD'), word_cnts)\n",
    "      event_procedures_ls.append(visit_procedures)\n",
    "\n",
    "\n",
    "  # Randomly shuffle diagnosis and procedure codes and append a time interval after, if available. Ensure the output sentence will not be more than 100 words.\n",
    "  arrs = []\n",
    "  word_cnt = 0\n",
    "  for i, vd in enumerate(event_diagnoses_ls):\n",
    "      arr = vd + event_procedures_ls[i]\n",
    "      random.shuffle(arr)\n",
    "      if i < len(time_interval_strs):\n",
    "          arr.append(time_interval_strs[i])\n",
    "\n",
    "      new_word_cnt = word_cnt + len(arr)\n",
    "\n",
    "      if new_word_cnt > 100:\n",
    "          # Calculate the number of elements needed to reach exactly 100 words\n",
    "          elements_needed = 100 - word_cnt\n",
    "\n",
    "          # Take a subset of arr to make new_word_cnt equal 100\n",
    "          arr = arr[:elements_needed]\n",
    "          arrs.append(arr)\n",
    "          break\n",
    "\n",
    "      arrs.append(arr)\n",
    "      word_cnt = new_word_cnt\n",
    "\n",
    "  # Combine all codes and time interval to create a phrase, representing a visit\n",
    "  phrases = [\" \".join(arr) for arr in arrs]\n",
    "\n",
    "  # Combine all phrases to create a sentence, representing a sequence as outlined by the paper\n",
    "  sentence = \" \".join(phrases)\n",
    "  sentences.append(sentence)\n",
    "\n",
    "# output to json file\n",
    "output_dir = \"data\"\n",
    "output_filename = \"sentences.json\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, output_filename), \"w\") as json_file:\n",
    "  json.dump(sentences, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6957a",
   "metadata": {},
   "source": [
    "### Test that output sentences satisfy the following conditions:\n",
    "- There is a sentence for each patient\n",
    "- Each sentence is capped to max 100 words\n",
    "- Multi visit patients have visits separated by a timestamp\n",
    "- Words should not exist in their ICD-10 form if used less than 100 times (should be replaced with RAREWORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b42736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# There should be one sentence per patient\n",
    "num_patients = len(mimic3_ds.patients)\n",
    "num_sentences = len(sentences)\n",
    "assert(num_patients == num_sentences)\n",
    "\n",
    "# There should be max 100 words per sentence\n",
    "word_lengths = map(lambda s: len(s.split()), sentences)\n",
    "assert(max(list(word_lengths)) <= 100)\n",
    "\n",
    "# There should be no word in any of the sentences that is present less than 100 times\n",
    "rarewords = [word for word, count in word_cnts.items() if count < 100]\n",
    "for sentence in sentences:\n",
    "  words_of_sentence = sentence.split()\n",
    "  rareword_violations = list(filter(lambda w: w in word_cnts and word_cnts[w] < 100, words_of_sentence))\n",
    "  assert(len(rareword_violations) == 0)\n",
    "\n",
    "\n",
    "# Patients with multiple visits should have timestamps separating their visits i.e. 1-3m or 12+m #TODO: Add TRANSFER to regex\n",
    "pattern = re.compile(r\"[-+]\")\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "    if len(p.visits) > 1:\n",
    "        if not pattern.search(sentences[i]):\n",
    "            print(f\"Failed assertion for sentences[{i}]: '{sentences[i]}'\")\n",
    "            assert(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f873178",
   "metadata": {},
   "source": [
    "## Training Word2Vec\n",
    "#TODO: Write description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdace4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def split_tokens(stemmer, stopwords, line):\n",
    "    return [\n",
    "        stemmer.stem(i)\n",
    "        for i in re.split(r\" +\", re.sub(r\"[^a-z@# ]\", \"\", line.lower()))\n",
    "        if (i not in stopwords) and len(i)\n",
    "    ]\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "sentences = [\n",
    "    \" \".join(sentence)\n",
    "    for line in open(\"data/shakespeare.txt\", \"r\").readlines()\n",
    "    if (sentence := split_tokens(stemmer, stopwords, line)) and sentence\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset, tok2id, wsize=3):\n",
    "        self.wsize = wsize\n",
    "        self.tok2id = tok2id\n",
    "\n",
    "        self.dataset = [\n",
    "            ctx\n",
    "            for sentence in dataset for ctx in self.get_contexts(sentence)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def get_contexts(self, sentence):\n",
    "        moving_window = []\n",
    "\n",
    "        for word_ix, word in enumerate(sentence):\n",
    "            target = self.tok2id[word]\n",
    "            window = [\n",
    "                word_ix + win_ix\n",
    "                for win_ix in range(-self.wsize, self.wsize + 1)\n",
    "                if (\n",
    "                    word_ix + win_ix >= 0\n",
    "                    and word_ix + win_ix < len(sentence)\n",
    "                    and win_ix != 0\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            moving_window += [\n",
    "                (target, self.tok2id[sentence[win_word]]) for win_word in window\n",
    "            ]\n",
    "\n",
    "        return moving_window\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.prediction = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.embedding(input)\n",
    "        logits = self.prediction(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "dataset = [sentence.split(\" \") for sentence in sentences]\n",
    "vocab = set([word for sentence in dataset for word in sentence])\n",
    "\n",
    "id2tok = dict(enumerate(vocab))\n",
    "tok2id = {token: id for id, token in id2tok.items()}\n",
    "\n",
    "word2vec_dataloader = DataLoader(\n",
    "    Word2VecDataset(dataset, tok2id),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "word2vec_model = Word2Vec(len(vocab), embedding_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "LR = 3e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(word2vec_model.parameters(), lr=LR)\n",
    "progress_bar = tqdm(range(EPOCHS * len(word2vec_dataloader)))\n",
    "\n",
    "running_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for center, context in word2vec_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = word2vec_model(input=context)\n",
    "        loss = loss_fn(logits, center)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    epoch_loss /= len(word2vec_dataloader)\n",
    "    running_loss.append(epoch_loss)\n",
    "\n",
    "plt.plot(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_distance_matrix(wordvecs, metric):\n",
    "    dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def get_k_similar_words(word, dist_matrix, k=10):\n",
    "    idx = tok2id[word]\n",
    "    dists = dist_matrix[idx]\n",
    "    ind = np.argpartition(dists, k)[:k+1]\n",
    "    ind = ind[np.argsort(dists[ind])][1:]\n",
    "    out = [(i, id2tok[i], dists[i]) for i in ind]\n",
    "    return out\n",
    "\n",
    "\n",
    "dmat = get_distance_matrix(\n",
    "    word2vec_model.prediction.weight.cpu().detach().numpy(),\n",
    "    \"cosine\"\n",
    ")\n",
    "\n",
    "test_sentence = [\"good\", \"father\" \"school\", \"hate\"]\n",
    "\n",
    "for word in test_sentence:\n",
    "    print(word, [token[1] for token in get_k_similar_words(word, dmat)], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec_model.state_dict()\n",
    "torch.save(weights, os.path.join(output_dir, \"word2vec.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef13fa",
   "metadata": {},
   "source": [
    "## Convolutional Network\n",
    "Now that Word2Vec has created embeddings for the sentences we generated in the first step, we are ready to pass the embeddings through a convolutional layer, followed by max pooling.\n",
    "\n",
    "First, we will generate the train and val loaders. This will first require us to create the validation set. In Deepr, we try to predict the likelihood of unplanned re-admission after both 3 and 6 months. Unplanned re-admission is coded in the database as an unplanned or emergency with the status not equal to \"elective\" or a transfer from another hospital.\n",
    "\n",
    "Next, we will define a model containing a convolutional network, ReLU and max pooling layer.\n",
    "\n",
    "Deepr reported the following parameters were determined to be optimal for the model:\n",
    "\n",
    "`m = 100, d = 1, motif size = 3, 4, and 5, n = 100 number of epochs = 10, minibatch size = 64, and l2 regularization λ = 1.0.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create validation dataset, train and val loaders for training Deepr.\n",
    "'''\n",
    "\n",
    "val_readmission_time_days = 180 # The threshold for flagging unplanned readmission for validation set\n",
    "risk_group = []\n",
    "remaining_group = [] #patients not put into the risk group\n",
    "for i, p in enumerate(mimic3_ds.patients.values()):\n",
    "\n",
    "  # Sort patient visits by encounter_time\n",
    "  sorted_visits = sorted(p.visits.items(), key=lambda v: v[1].encounter_time) # sort by encounter time in order to guage interval between visits\n",
    "\n",
    "  # Generate timestamps in between visits\n",
    "  discharge_times = [visit[1].discharge_time for visit in sorted_visits[:-1]]\n",
    "  encounter_times = [visit[1].encounter_time for visit in sorted_visits[1:]]\n",
    "\n",
    "  time_intervals = [\n",
    "      t2 - t1\n",
    "      for t1, t2 in zip(discharge_times, encounter_times)\n",
    "  ]\n",
    "  visit_types = [visit[1].attr_dict[\"ADMISSION_TYPE\"] for visit in sorted_visits[1:]]\n",
    "\n",
    "  unplanned_readmissions = [\n",
    "      i_vt\n",
    "      for i_vt, (interval, visit_type) in enumerate(zip(time_intervals, visit_types))\n",
    "      if interval.days <= val_readmission_time_days and visit_type == \"EMERGENCY\"\n",
    "  ]\n",
    "\n",
    "  if unplanned_readmissions:\n",
    "    risk_group.append(sentences[i])\n",
    "  else: \n",
    "    remaining_group.append(sentences[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk group patients should be separate from the candidate group, and sum to the length of the total dataset\n",
    "assert(len(mimic3_ds.patients.values()) == len(risk_group) + len(remaining_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "'''\n",
    "Dataset for Deepr model. __getitem__ returns the word indices for a given sentence. \n",
    "DeeprModel will then look up the word embeddings by the index in the embedding table.\n",
    "'''\n",
    "\n",
    "class DeeprDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, word2index):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.word2index = word2index\n",
    "\n",
    "    def get_indices(self, sentence):\n",
    "        return torch.tensor([self.word2index[word] for word in sentence.split()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.get_indices(self.x_data[idx])\n",
    "        y = self.y_data[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The sentences in each batch will likely not all be the same size, and thus we need to pad the sentences. \n",
    "DeeprDataset returns the indicies of the words in each sentence. We will padd these vectors of indicies with 0s.\n",
    "It is important later on ensure that index 0 of the embedding table is a torch.zero(embedding_dim), as these are just placeholder values,\n",
    "and to mask them out later on.\n",
    "'''\n",
    "\n",
    "def pad_collate(batch):\n",
    "    x_data, y_data = zip(*batch)\n",
    "    x_data_padded = pad_sequence(x_data, batch_first=True, padding_value=0)\n",
    "    # Create binary mask\n",
    "    mask = (x_data_padded != 0).float()\n",
    "\n",
    "    # Convert from tuple to tensor\n",
    "    y_data_tensor = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "    return x_data_padded, y_data_tensor, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "'''\n",
    "Generate the training and validation sets. The training data is a 1-1 mixture of risk and control patients, where x \n",
    "is the sentence of the patient, and y is the corresponding binary label of readmission (1=readmit, 0=non-readmit)\n",
    "\n",
    "We also ensure that index 0 of the embedding table is associated with the word \"<pad>\", so that the padding index has an associated word, \n",
    "as it was not learned in Word2Vec, but will be used in the Deepr model later on.\n",
    "'''\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "control_group = random.sample(remaining_group, len(risk_group))\n",
    "control_labels = [0] * len(control_group)\n",
    "control_zip = list(zip(control_group, control_labels))\n",
    "\n",
    "risk_labels = [1] * len(risk_group)\n",
    "risk_zip = list(zip(risk_group, risk_labels))\n",
    "\n",
    "all_zip = list(control_zip + risk_zip)\n",
    "random.shuffle(all_zip)\n",
    "\n",
    "train_dataset = all_zip[:int(len(all_zip)*train_ratio)]\n",
    "val_dataset = all_zip[int(len(all_zip)*train_ratio):]\n",
    "\n",
    "x_train, y_train = zip(*train_dataset)\n",
    "x_val, y_val = zip(*val_dataset)\n",
    "\n",
    "# Shift all elements up, and add the padding word at index 0\n",
    "word2index = {word: idx+1 for idx, word in enumerate(tok2id)}\n",
    "word2index[\"<pad>\"] = 0\n",
    "\n",
    "train_dataset = DeeprDataset(x_train, y_train, word2index)\n",
    "val_dataset = DeeprDataset(x_val, y_val, word2index)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e270c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "DeeprModel does the following things:\n",
    "\n",
    "1. During initialization, updates the embedding table learned from Word2Vec to include the embedding for padding.\n",
    "2. Pretrains an embedding layer with the weights from Word2Vec.\n",
    "3. Initializes the model with hyperparameters from the Deepr paper, mentioned at the beginning of this section.\n",
    "4. Passes the embeddings through convolutional, max pooling and a linear classifier.\n",
    "'''\n",
    "\n",
    "class DeeprModel(nn.Module):\n",
    "    def __init__(self, word_embeddings_dict, word2index, embedding_dim, num_filters, window_size, num_classes):\n",
    "        super(DeeprModel, self).__init__()\n",
    "\n",
    "        # Prepare the embedding matrix; this takes the ouput of word2vec but inserts \"<pad>\" string, assigning an empty embedding to it\n",
    "        embedding_matrix = torch.zeros(len(word_embeddings_dict) + 1, embedding_dim)\n",
    "        for word, index in word2index.items():\n",
    "            if word != \"<pad>\":\n",
    "                embedding_matrix[index] = torch.tensor(word_embeddings_dict[word])\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(embedding_dim, num_filters, window_size)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        x = x.transpose(1, 2)  # conv requires input_channels to be dim 1. Shape: (batch_size, embedding_dim, sequence_length)\n",
    "\n",
    "        # Add padding to x, because convolution will reduce dimensionality and make multiplication with the mask impossible due to size mismatch\n",
    "        padding_size = self.conv.kernel_size[0] - 1\n",
    "        x = F.pad(x, (padding_size, 0))  # Pad the input tensor on the left side\n",
    "\n",
    "        x = F.relu(self.conv(x))  # Shape: (batch_size, num_filters, sequence_length)\n",
    "        x = x * mask.unsqueeze(1) # Apply mask\n",
    "        x = self.pool(x).squeeze(2)  # Shape: (batch_size, num_filters)\n",
    "\n",
    "        x = self.fc(x)  # Shape: (batch_size, num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb756e",
   "metadata": {},
   "source": [
    "## Training and validating DeeprModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bf065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "'''\n",
    "We first initialize the model's hyperparameters. Then, we load the saved weights from Word2Vec into a new word2vec_model instance. \n",
    "We initialize a DeeprModel instance, CrossEntropyLoss function, and SGD optimizer, the later two being specified in the Deepr paper.\n",
    "In the training loop, we compare the logits computed in the forward method of the model against the labels we computed in other steps,\n",
    "compute the loss, back propogate and update weights.\n",
    "\n",
    "In validating the data, we compute validation loss and AUC.\n",
    "\n",
    "'''\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "l2_reg = 1.0\n",
    "\n",
    "# Initialize the model\n",
    "num_filters = 64\n",
    "filter_size = 3\n",
    "num_classes = 2  # For binary classification\n",
    "embedding_dim = 100\n",
    "\n",
    "# Load weights from .pt file\n",
    "vocab_size = len(vocab)\n",
    "word2vec_model = Word2Vec(vocab_size, embedding_dim)\n",
    "weights_path = os.path.join(output_dir, \"word2vec.pt\")\n",
    "saved_weights = torch.load(weights_path)\n",
    "\n",
    "word2vec_model.load_state_dict(saved_weights)\n",
    "\n",
    "word_embeddings_dict = {\n",
    "    word: word2vec_model.embedding(torch.tensor(tok2id[word])).detach().numpy()\n",
    "    for word in tok2id\n",
    "}\n",
    "\n",
    "model = DeeprModel(word_embeddings_dict, word2index, embedding_dim, num_filters, filter_size, num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y, mask in train_dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch_x, mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch_x, batch_y, mask in val_dataloader:\n",
    "            logits = model(batch_x, mask)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Get probabilities for the positive class\n",
    "            probabilities = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            all_logits.extend(probabilities)\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        auc = roc_auc_score(all_labels, all_logits)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}, AUC: {auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
